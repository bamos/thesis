\PassOptionsToPackage{svgnames,dvipsnames}{xcolor}

\documentclass[12pt]{cmuthesis}

\usepackage[Lenny]{fncychap}
\ChNameVar{\Large}

\input{sections/packages}
\input{sections/macros}

% \draftstamp{\today}{DRAFT}

\begin {document}
\frontmatter

\pagestyle{empty}

\title{{\bf Differentiable Optimization-Based Modeling for Machine Learning}}
\author{Brandon Amos}
\date{May 2019}
\Year{2019}
\trnumber{CMU-CS-19-109}

\committee{
\begin{tabular}{rl}
J. Zico Kolter, Chair & \textit{Carnegie Mellon University} \\
Barnab{\'a}s P{\'o}czos & \textit{Carnegie Mellon University} \\
Jeff Schneider & \textit{Carnegie Mellon University} \\
Vladlen Koltun & \textit{Intel Labs} \\
\end{tabular}
}

\support{}
\disclaimer{}

\keywords{machine learning, statistical modeling,
  convex optimization, deep learning, control,
  reinforcement learning}

\maketitle

\begin{dedication}
  To all of the people that light up my life. {\ensuremath\heartsuit}
\end{dedication}

\begin{abstract}
  Domain-specific modeling priors and specialized components are
  becoming increasingly important to the machine learning field.
  These components integrate specialized knowledge that we have
  as humans into model.
  We argue in this thesis that optimization methods provide an
  expressive set of operations that should be part of the
  machine learning practitioner's modeling toolbox.

  We present two foundational approaches for optimization-based modeling:
  1) the \emph{OptNet} architecture that integrates
  optimization problems as individual layers in larger end-to-end
  trainable deep networks, and
  2) the \emph{input-convex neural network (ICNN)}
  architecture that helps make inference and learning in deep
  energy-based models and structured prediction more tractable.

  We then show how to use the OptNet approach
  1) as a way of combining model-free and model-based reinforcement
  learning and
  2) for top-$k$ learning problems.
  We conclude by showing how to differentiate cone programs
  and turn the \cvxpy domain specific language into
  a differentiable optimization layer that enables rapid prototyping of
  the approaches in this thesis. \\

  \noindent
  The source code for this thesis document is available in open source form at:
  \begin{center}
  \url{https://github.com/bamos/thesis}
  \end{center}
\end{abstract}

% \newgeometry{left=0.5in,right=0.5in,top=1in,bottom=1.4in}
\begin{acknowledgments}
  I have been incredibly fortunate and privileged throughout
  my entire life to have been given many opportunities
  that have led me to pursue this thesis research.
  Thanks to the thousands of people in the universe throughout
  the past few millennia who have provided me with the
  foundation, environment, safety, health, support, service,
  financial well-being, love, joy, knowledge, kindness, calmness,
  and happiness to produce this work.

  This thesis would not have been possible without the close
  collaboration I have had with my advisor J.~Zico Kolter over
  the past few years.
  Zico's creativity and passion have profoundly shaped
  the way I think about academic problems and pursue
  research directions, and more broadly I have learned much
  more from him along the way.
  I am incredibly grateful for the immense
  amount of time and energy Zico has put into shaping the
  direction of this work and for molding me into who I am.

  Thanks to all of my close collaborators who have contributed
  to projects appearing in this thesis, including
  Byron Boots, Ivan Jimenez, Vladlen Koltun, Jacob Sacks, and Lei Xu,
  and more recently
  Akshay Agrawal,
  Shane Barratt,
  Stephen Boyd,
  Steven Diamond,
  and Brendan O'Donoghue.

  This thesis was also made possible by the great research
  environment that CMU has provided me during my studies here.
  CMU's collaborative, thriving, and understanding environment gave
  me the true capabilities to pursue my passions throughout my time here.
  I spent my first two years honing my systems skills working on
  wearable cognitive assistance applications with
  Mahadev (Satya) Satyanarayanan and am
  indebted to him for kindly giving me the freedom to pursue my
  interests in machine learning while part of his systems group.
  I hope that someday I will be able to pay this kindness forward.
  Thanks also to all of the administrative staff that have
  kept everything at CMU running smoothly, including
  Deb Cavlovich and Ann Stetser.
  I am also very thankful to Gaurav Manek for a well-engineered
  cluster setup that has made running and managing
  experiments effortless for the rest of us.
  And thanks to everybody else at CMU who have made
  graduate school incredibly enjoyable.
  These wonderful memories will stay with me for life.
  This includes
  Maruan Al-Shedivat,
  Alnur Ali,
  Filipe de Avila Belbute-Peres,
  Shaojie Bai,
  Sol Boucher,
  Noam Brown,
  Volkan Cirik,
  Dominic Chen,
  Zhuo Chen,
  Michael Coblenz,
  Jeremy Cohen,
  Jonathan Dinu,
  Priya Donti,
  Gabriele Farina,
  Benjamin Gilbert,
  Kiryong Ha,
  Jan Harkes,
  Wenlu Hu,
  Roger Iyengar,
  Christian Kroer,
  Jonathan Laurent,
  Jay-Yoon Lee,
  Lisa Lee,
  Chun Kai Ling,
  Stefan Muller,
  Vaishnavh Nagarajan,
  Vittorio Perera,
  Padmanabhan (Babu) Pillai,
  George Philipp,
  Aurick Qiao,
  Leslie Rice,
  Wolf Richter,
  Mel Roderick,
  Petar Stojanov,
  Dougal Sutherland,
  Junjue Wang,
  Phillip Wang,
  Po-Wei Wang,
  Josh Williams,
  Ezra Winston,
  Eric Wong,
  Han Zhao, and
  Xiao Zhang.

  My Ph.D.~would have been severely lacking without my internships
  at DeepMind in 2017 and Intel Labs in 2018.
  I learned how to craft large-scale reinforcement learning systems
  from Nando de Freitas and Misha Denil at DeepMind and
  about cutting-edge vision research from
  Vladlen Koltun at Intel Labs.
  Thank you all for hosting me.
  I am also grateful for all of the conversations and collaborations
  with the other interns and researchers in the industry as well,
  including
  Yannis Assael,
  David Budden,
  Serkan Cabi,
  Kris Cao,
  Chen Chen,
  Qifeng Chen,
  Yutian Chen,
  Mike Chrzanowski,
  Sergio Gomez Colmenarejo,
  Tim Cooijmans,
  Soham De,
  Laurent Dinh,
  Vincent Dumoulin,
  Tom Erez,
  Michael Figurnov,
  Jakob Foerster,
  Marco Fraccaro,
  Yaroslav Ganin,
  Katelyn Gao,
  Yang Gao,
  Caglar Gulcehre,
  Karol Hausman,
  Matthew W.~Hoffman,
  Drew Jaegle,
  David Lindell,
  Hanxiao Liu,
  Simon Kohl,
  Alistair Muldal,
  Alexander Novikov,
  Tom Le Paine,
  Ben Poole,
  Rene Ranftl,
  Scott Reed,
  German Ros,
  Evan Shelhamer,
  Sainbayar Sukhbaatar,
  Casper Kaae SÃ¸nderby,
  Brendan Shillingford,
  Yuval Tassa,
  Jonathan Uesato,
  Ziyu Wang,
  Abhay Yadav,
  Xuaner Zhang, and
  Yuke Zhu.

  I am grateful to the broader machine learning research community
  that has been thriving throughout my studies and has
  supported the direction of this work.
  This includes the Caffe, PyTorch, and TensorFlow communities
  I have interacted with over the years.
  These ecosystems have made the implementation and engineering
  side of this thesis easy and enjoyable.
  Thanks especially to Soumith Chintala, Adam Paszke, and the rest
  of the (Py)Torch community for helping me debug many strange
  errors and eventually contribute back.
  And thanks to everybody in the broader machine learning community
  who has given me deeper insights into problems or has graciously
  helped me with their code, including
  David Belanger,
  Alfredo Canziani,
  Alex Terenin, and
  Rowan Zellers.

  Thanks to all of the other communities that have provided me
  with the tooling and infrastructure necessary that allows
  me to work comfortably. These communities deserve more credit
  for the impacts that they have and the immense amount of
  development effort behind them and include the
  emacs \citep{stallman1981emacs},
  git \citep{torvalds2005git},
  hammerspoon,
  homebrew,
  \LaTeX \citep{lamport1994latex},
  Linux,
  mjolnir,
  mu4e,
  mutt,
  tmux,
  vim,
  xmonad \citep{stewart2007xmonad}, and
  zsh projects,
  as well as the many pieces of the Python ecosystem
  \citep{van1995python,oliphant2007python}, especially
  Jupyter \citep{kluyver2016jupyter},
  Matplotlib \citep{hunter2007matplotlib},
  seaborn,
  numpy \citep{van2011numpy},
  pandas \citep{mckinney2012python}, and
  SciPy \citep{jones2014scipy}.

  Looking back, my teachers and mentors earlier in my life
  ignited my interests in mathematics and computer science
  and opened my eyes.
  My high school teachers
  Suzanne Nicewonder,
  Susheela Shanta, and
  Janet Washington gave me a solid foundation
  in engineering and mathematics.
  Mack McGhee at Sunapsys hosted me for an
  internship that introduced to the wonderful
  world of Linux.
  Moving into my undergrad,
  Layne T.~Watson and David Easterling
  introduced me to the beautiful fields
  of optimization, numerical methods, and
  high-performance computing, and taught me how to
  write extremely optimized and robust Fortran code.
  I apologize for going to the dark side and writing
  ANTODL (another thesis on deep learning).
  Jules White and Hamilton Turner taught me how
  to hack Android internals and architect awesome Scala code.
  Binoy Ravindran, Alastair Murray, and Rob Lyerly
  taught me how to hack on compilers
  and the Linux kernel.

  On the personal side, I would like to thank all of my
  other friends, family members, and partners that
  have provided me with an immense amount of love,
  support, and encouragement throughout the years,
  especially Alice, Emma, and Nil-Jana.
  Thanks to my parents Sandy and David;
  brothers Chad and Chase;
  grandparents Candyth, Marshall, and Geneva;
  and the rest of my extended family
  for raising me in a wonderful environment and
  encouraging me at every step along the way.
  Thanks to my uncle Dan Dunlap for inspiring me and
  raving about AI, CS, philosophy, and music all of these years.
  And thanks to everybody else I have met in the
  arts,
  board games,
  climbing,
  cycling,
  dance,
  lifting,
  meditation,
  music,
  nature,
  poetry,
  theatre, and
  yoga
  communities in Pittsburgh, San Francisco, and London for
  providing a near-infinite amount of distractions from
  this thesis.
\end{acknowledgments}
% \restoregeometry

\pagestyle{plain}

\tableofcontents
\addtocontents{toc}{\vspace*{-2cm}}
\listoffigures
\addtocontents{lof}{\vspace*{-2cm}}
\listoftables
\listofalgorithms

\mainmatter

\include{sections/intro}
\include{sections/background}

\part{Foundations}
\include{sections/optnet}
\include{sections/icnn}

\part{Extensions and Applications}
\include{sections/empc}
\include{sections/lml}
\include{sections/cvxpyth}

\part{Conclusions and Future Directions}
\include{sections/conclusions}

\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{Bibliography}

\vspace{-25mm}
This bibliography contains \total{citenum} references.
\vspace{10mm}

\printbibliography[heading=none]

\end{document}

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: xetex
%%% End:
